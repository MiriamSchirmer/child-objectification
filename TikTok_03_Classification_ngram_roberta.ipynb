{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1snW-YGYCx60H-nkuqnLyehDuTRqk_ObZ","timestamp":1740715391520}],"authorship_tag":"ABX9TyOb1UwGNgXh1u+Bacz/Zc7j"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**TRAIN OBJECTIFICATION CLASSIFIER**"],"metadata":{"id":"aviL6wynGyxt"}},{"cell_type":"code","source":["pip install scikeras\n"],"metadata":{"id":"t4SxN18FgYz1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**1. NGRAM**"],"metadata":{"id":"XjXUBH8Rl69x"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset"],"metadata":{"id":"-tVFmnsbgPmg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load dataset\n","# Ensure the dataset is in a DataFrame with 'text', 'neutral', and 'negative' columns\n","data = pd.read_csv(\"/INSERT-DATA-PATH.csv\")\n"],"metadata":{"id":"7Yu4CoTN6NJ2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data[['neutral', 'negative']] = data[['neutral', 'negative']].fillna(0).astype(int)"],"metadata":{"id":"3-uyoDfd6VRc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(data['neutral'].unique())\n","print(data['negative'].unique())\n"],"metadata":{"id":"F_zJhuRv6PJ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define TF-IDF vectorizer with N-grams (1,2,3)\n","vectorizer = TfidfVectorizer(ngram_range=(1,3), max_features=5000)\n","X = vectorizer.fit_transform(data['text']).toarray()\n","y_neutral = data['neutral'].values\n","y_negative = data['negative'].values\n","\n","# Convert to PyTorch tensors\n","X_tensor = torch.tensor(X, dtype=torch.float32)\n","y_neutral_tensor = torch.tensor(y_neutral, dtype=torch.float32)\n","y_negative_tensor = torch.tensor(y_negative, dtype=torch.float32)\n","\n","# Define PyTorch neural network class\n","class TextClassifier(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, dropout_rate):\n","        super(TextClassifier, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n","        self.fc3 = nn.Linear(hidden_dim // 2, 1)\n","        self.relu = nn.ReLU()\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = self.relu(self.fc2(x))\n","        x = self.dropout(x)\n","        x = self.sigmoid(self.fc3(x))\n","        return x\n","\n","# Perform 5-Fold Cross Validation\n","kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# Store best params\n","best_params = {}\n","\n","# Function to train and evaluate model\n","def train_and_evaluate(y_tensor, label_name):\n","    input_dim = X.shape[1]\n","    hidden_dim = 128\n","    dropout_rate = 0.3\n","    batch_size = 32\n","    epochs = 10\n","\n","    dataset = TensorDataset(X_tensor, y_tensor.unsqueeze(1))\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","    model = TextClassifier(input_dim, hidden_dim, dropout_rate)\n","    criterion = nn.BCELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        for batch_X, batch_y in dataloader:\n","            optimizer.zero_grad()\n","            outputs = model(batch_X).squeeze()\n","            loss = criterion(outputs, batch_y.squeeze())\n","            loss.backward()\n","            optimizer.step()\n","\n","    model.eval()\n","    with torch.no_grad():\n","        y_pred_proba = model(X_tensor).squeeze().numpy()\n","\n","    y_pred = (y_pred_proba > 0.5).astype(int)\n","\n","    # Metrics\n","    roc_auc = roc_auc_score(y_tensor.numpy(), y_pred_proba)\n","    acc = accuracy_score(y_tensor.numpy(), y_pred)\n","    f1 = f1_score(y_tensor.numpy(), y_pred)\n","    precision = precision_score(y_tensor.numpy(), y_pred)\n","    recall = recall_score(y_tensor.numpy(), y_pred)\n","    cm = confusion_matrix(y_tensor.numpy(), y_pred)\n","\n","    # Store best params\n","    best_params[label_name] = {\n","        \"hidden_dim\": hidden_dim,\n","        \"dropout_rate\": dropout_rate,\n","        \"batch_size\": batch_size,\n","        \"epochs\": epochs\n","    }\n","\n","    # Display results\n","    print(f\"{label_name} Classification Results:\")\n","    print(f\"ROC AUC: {roc_auc:.4f}\")\n","    print(f\"Accuracy: {acc:.4f}\")\n","    print(f\"F1 Score: {f1:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(\"Confusion Matrix:\")\n","    print(cm)\n","\n","\n","# Train models separately for 'neutral' and 'negative'\n","train_and_evaluate(y_neutral_tensor, \"Neutral\")\n","train_and_evaluate(y_negative_tensor, \"Negative\")\n","\n","# Print best parameters\n","print(\"Best Parameters for Each Model:\")\n","for label, params in best_params.items():\n","    print(f\"{label}: {params}\")\n","\n","\n"],"metadata":{"id":"XNRpjQf1G6gg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**2. ROBERTA**"],"metadata":{"id":"_4VHFcxWlyI6"}},{"cell_type":"code","source":["# Load dataset\n","data = pd.read_excel(\"/INSERT-DATA-PATH\")\n","data[['neutral', 'negative']] = data[['neutral', 'negative']].fillna(0).astype(int)\n","print(data['neutral'].unique())\n","print(data['negative'].unique())"],"metadata":{"id":"80Vsg_NEmiuJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740276017756,"user_tz":360,"elapsed":273,"user":{"displayName":"Miriam Schirmer","userId":"16170523396349156911"}},"outputId":"1407629d-58c7-43df-b1b0-0115d564b3e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1 0]\n","[1 0]\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import RobertaTokenizer, RobertaModel\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import os\n","import warnings\n","import json\n","from tqdm import tqdm\n","\n","# **Suppress Hugging Face Warnings**\n","warnings.filterwarnings(\"ignore\", message=\"Some weights of RobertaModel were not initialized\")\n","\n","\n","# Data Loading and Preparation\n","\n","# Convert labels to float for BCELoss\n","y_neutral = data['neutral'].values.astype(float)\n","y_negative = data['negative'].values.astype(float)\n","\n","tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n","\n","class TextDataset(Dataset):\n","    def __init__(self, texts, labels):\n","        self.texts = texts\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        encoding = tokenizer(\n","            self.texts[idx],\n","            truncation=True,\n","            padding='max_length',\n","            max_length=128,\n","            return_tensors=\"pt\"\n","        )\n","        return {\n","            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n","            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n","            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float32),\n","            \"text\": self.texts[idx]\n","        }\n","\n","train_texts_neutral, test_texts_neutral, train_neutral, test_neutral = train_test_split(\n","    data[\"text\"], y_neutral, test_size=0.2, random_state=42)\n","train_texts_negative, test_texts_negative, train_negative, test_negative = train_test_split(\n","    data[\"text\"], y_negative, test_size=0.2, random_state=42)\n","\n","train_dataset_neutral = TextDataset(train_texts_neutral.tolist(), train_neutral)\n","test_dataset_neutral = TextDataset(test_texts_neutral.tolist(), test_neutral)\n","train_dataset_negative = TextDataset(train_texts_negative.tolist(), train_negative)\n","test_dataset_negative = TextDataset(test_texts_negative.tolist(), test_negative)\n","\n","\n","# Define the Model\n","\n","class RobertaClassifier(nn.Module):\n","    def __init__(self, dropout_rate=0.3):\n","        super(RobertaClassifier, self).__init__()\n","        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.fc = nn.Linear(self.roberta.config.hidden_size, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n","        x = self.dropout(outputs.pooler_output)\n","        x = self.fc(x)\n","        return self.sigmoid(x).squeeze()\n","\n","\n","# Hyperparameter Grid\n","\n","param_grid = {\n","    \"dropout_rate\": [0.3, 0.4],\n","    \"learning_rate\": [2e-5, 3e-5],\n","    \"epochs\": [2, 3]\n","}\n","\n","\n","# Training Function with Debugging\n","\n","def train_final_model(train_dataset, test_dataset, model_name, param_grid, global_pbar):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    print(f\"\\nðŸš€ Starting training for {model_name} on {device}\")\n","\n","    best_model = None\n","    best_loss = float(\"inf\")\n","    best_params = None\n","    misclassified_samples = []\n","\n","    for dropout_rate in param_grid[\"dropout_rate\"]:\n","        for learning_rate in param_grid[\"learning_rate\"]:\n","            for epochs in param_grid[\"epochs\"]:\n","                print(f\"\\nðŸ”¹ Training {model_name} with dropout={dropout_rate}, lr={learning_rate}, epochs={epochs}\")\n","                model = RobertaClassifier(dropout_rate=dropout_rate).to(device)\n","                optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","                criterion = nn.BCELoss()\n","                train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","                # Debug: Check if DataLoader is working\n","                first_batch = next(iter(train_loader))\n","                print(f\"ðŸ” Sample Batch: Input IDs Shape: {first_batch['input_ids'].shape}, Labels: {first_batch['labels'][:5]}\")\n","\n","                for epoch in range(epochs):\n","                    model.train()\n","                    total_loss = 0\n","                    for batch in train_loader:\n","                        input_ids = batch[\"input_ids\"].to(device)\n","                        attention_mask = batch[\"attention_mask\"].to(device)\n","                        labels_batch = batch[\"labels\"].to(device)\n","\n","                        optimizer.zero_grad()\n","                        outputs = model(input_ids, attention_mask)\n","\n","                        loss = criterion(outputs, labels_batch)\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                        total_loss += loss.item()\n","                        global_pbar.update(1)\n","\n","                        # Debug: Print loss\n","                        if global_pbar.n % 100 == 0:\n","                            print(f\"âš¡ Loss: {loss.item():.4f}\")\n","\n","                    avg_train_loss = total_loss / len(train_loader)\n","\n","                    # Evaluate on test set\n","                    model.eval()\n","                    correct, total = 0, 0\n","                    misclassified = []\n","                    with torch.no_grad():\n","                        test_loader = DataLoader(test_dataset, batch_size=16)\n","                        for batch in test_loader:\n","                            input_ids = batch[\"input_ids\"].to(device)\n","                            attention_mask = batch[\"attention_mask\"].to(device)\n","                            labels_batch = batch[\"labels\"].to(device)\n","\n","                            outputs = model(input_ids, attention_mask)\n","                            preds = (outputs > 0.5).float()\n","\n","                            total += labels_batch.size(0)\n","                            correct += (preds == labels_batch).sum().item()\n","\n","                            # Log misclassified samples\n","                            for i in range(len(preds)):\n","                                if preds[i] != labels_batch[i]:\n","                                    misclassified.append(batch[\"text\"][i])\n","\n","                    accuracy = correct / total\n","                    avg_test_loss = total_loss / len(test_loader)\n","\n","                    # Save best model\n","                    if avg_test_loss < best_loss:\n","                        best_loss = avg_test_loss\n","                        best_model = model.state_dict()\n","                        best_params = {\n","                            \"dropout_rate\": dropout_rate,\n","                            \"learning_rate\": learning_rate,\n","                            \"epochs\": epochs\n","                        }\n","                        misclassified_samples = misclassified[:3]\n","\n","\n","    # Save best model\n","    model_save_path = f\"{save_dir}/{model_name}_best.pth\"\n","    torch.save(best_model, model_save_path)\n","\n","    # Save misclassified samples\n","    misclassified_save_path = f\"{save_dir}/{model_name}_misclassified.json\"\n","    with open(misclassified_save_path, \"w\") as f:\n","        json.dump(misclassified_samples, f, indent=4)\n","\n","    return best_params\n","\n","\n","# Training\n","\n","\n","save_dir = \"/INSERT-PATH\"\n","os.makedirs(save_dir, exist_ok=True)\n","\n","total_batches = (\n","    (len(train_dataset_neutral) + len(train_dataset_negative)) // 16\n","    * sum(param_grid[\"epochs\"]) * len(param_grid[\"dropout_rate\"]) * len(param_grid[\"learning_rate\"])\n",")\n","\n","with tqdm(total=total_batches, desc=\"Overall Training Progress\", unit=\"batch\") as global_pbar:\n","    best_params_neutral = train_final_model(train_dataset_neutral, test_dataset_neutral, \"Neutral_Classifier\", param_grid, global_pbar)\n","    best_params_negative = train_final_model(train_dataset_negative, test_dataset_negative, \"Negative_Classifier\", param_grid, global_pbar)\n","\n"],"metadata":{"id":"_ztFzkB497Oc","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import json\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_auc_score\n","from transformers import RobertaTokenizer\n","\n","# Load the tokenizer\n","tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n","\n","# Load dataset for testing\n","class TextDataset(Dataset):\n","    def __init__(self, texts, labels):\n","        self.texts = texts\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        encoding = tokenizer(\n","            self.texts[idx],\n","            truncation=True,\n","            padding='max_length',\n","            max_length=128,\n","            return_tensors=\"pt\"\n","        )\n","        return {\n","            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n","            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n","            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float32),\n","            \"text\": self.texts[idx]\n","        }\n","\n","# Load test datasets\n","test_dataset_neutral = TextDataset(test_texts_neutral.tolist(), test_neutral)\n","test_dataset_negative = TextDataset(test_texts_negative.tolist(), test_negative)\n","\n","def evaluate_model(model_path, test_dataset, model_name):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Load best model\n","    model = RobertaClassifier()\n","    model.load_state_dict(torch.load(model_path, map_location=device))\n","    model.to(device)\n","    model.eval()\n","\n","    test_loader = DataLoader(test_dataset, batch_size=16)\n","    all_preds, all_labels, all_probs, misclassified = [], [], [], []\n","\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            labels_batch = batch[\"labels\"].cpu().numpy()\n","\n","            outputs = model(input_ids, attention_mask)\n","            probs = outputs.cpu().numpy()  # Raw probabilities before thresholding\n","            preds = (probs > 0.5).astype(int)  # Convert to binary predictions\n","\n","            all_preds.extend(preds)\n","            all_probs.extend(probs)\n","            all_labels.extend(labels_batch)\n","\n","            # Store misclassified samples\n","            for i in range(len(preds)):\n","                if preds[i] != labels_batch[i]:\n","                    misclassified.append({\n","                        \"text\": batch[\"text\"][i],\n","                        \"true_label\": int(labels_batch[i]),\n","                        \"predicted_label\": int(preds[i]),\n","                        \"predicted_prob\": float(probs[i])  # Include probability score\n","                    })\n","\n","    # Convert lists to numpy arrays\n","    all_preds = np.array(all_preds)\n","    all_probs = np.array(all_probs).flatten()\n","    all_labels = np.array(all_labels)\n","\n","    # Compute Metrics\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\")\n","    conf_matrix = confusion_matrix(all_labels, all_preds)\n","    roc_auc = roc_auc_score(all_labels, all_probs)\n","\n","    # Print Metrics\n","    print(f\"Final Classification Metrics for {model_name}\")\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1-score: {f1:.4f}\")\n","    print(f\"ROC AUC Score: {roc_auc:.4f}\")\n","    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n","\n","\n","\n","# Evaluate models & print misclassified samples\n","evaluate_model(\"/MODEL-PATH\", test_dataset_neutral, \"Neutral_Classifier\")\n","evaluate_model(\"/MODEL-PATH\", test_dataset_negative, \"Negative_Classifier\")\n"],"metadata":{"id":"Bfmb-5J8ndtL"},"execution_count":null,"outputs":[]}]}