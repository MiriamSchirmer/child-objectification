{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*This notebook implements BERTopic for TikTok comments on child-featuring videos.*\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-yb0jILDlTuj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install relevant libraries"
      ],
      "metadata": {
        "id": "qQpLXk5TljTs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9DQ91lfIBXg"
      },
      "outputs": [],
      "source": [
        "!pip install -q bertopic umap-learn hdbscan scikit-learn transformers sentence-transformers tqdm polars\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cQEQdy1KEmY"
      },
      "source": [
        "Run everything until here to start new runtime!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRO0KsJENsst"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"/INSERT-DATA-PATH.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "texts = df['text'].tolist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove comments that are not meaningful for TM, such as mentions"
      ],
      "metadata": {
        "id": "ZsokuNURl3rV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxmZypooWFa2"
      },
      "outputs": [],
      "source": [
        "# Define regex pattern to match ONLY mentions (like \"@name\", \"@user123\")\n",
        "mention_pattern = r\"^@\\w+$\"\n",
        "\n",
        "# Remove rows where the comment consists of only a mention\n",
        "df = df[~df['text'].str.match(mention_pattern, na=False)]\n",
        "\n",
        "# Convert to list\n",
        "texts = df['text'].tolist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embed comments"
      ],
      "metadata": {
        "id": "OB_siAL9mAY6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kBXE1qyPPDL"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load a pre-trained sentence transformer model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Ensure all texts are strings and replace NaNs with an empty string\n",
        "texts = [str(text) if isinstance(text, (str, int, float)) else \"\" for text in texts]\n",
        "\n",
        "# Generate embeddings with progress tracking\n",
        "embeddings = []\n",
        "for text in tqdm(texts, desc=\"Generating embeddings\"):\n",
        "    embedding = embedding_model.encode(text)\n",
        "    embeddings.append(embedding)\n",
        "\n",
        "# Convert to numpy array\n",
        "embeddings = np.array(embeddings)\n",
        "\n",
        "# Save embeddings for later reuse\n",
        "np.save(f\"{SAVE_PATH}/text_embeddings.npy\", embeddings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving embeddings separately to reload at a later stage (optional)"
      ],
      "metadata": {
        "id": "2IQSghp3mKLP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNSywHMrP4SB"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Save the cleaned text list as a JSON file\n",
        "with open(f\"{SAVE_PATH}/cleaned_texts.json\", \"w\") as f:\n",
        "    json.dump(texts, f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run TM"
      ],
      "metadata": {
        "id": "uw3KnMdvmSBU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7hw7eP7aw_u"
      },
      "outputs": [],
      "source": [
        "from bertopic import BERTopic\n",
        "import umap\n",
        "\n",
        "# Initialize UMAP for dimensionality reduction\n",
        "umap_model = umap.UMAP(n_neighbors=15, n_components=5, metric='cosine')\n",
        "\n",
        "# Fit BERTopic model\n",
        "topic_model = BERTopic(umap_model=umap_model, embedding_model=\"all-MiniLM-L6-v2\", verbose=True)\n",
        "topics, probs = topic_model.fit_transform(texts, embeddings)\n",
        "\n",
        "# Save topic assignments to DataFrame\n",
        "df['topic'] = topics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save model and toppic assignments (optional)"
      ],
      "metadata": {
        "id": "gc8e4ynKmYOs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccNtXSeJa2t4"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Ensure directory exists\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "\n",
        "# Save BERTopic model\n",
        "topic_model.save(f\"{SAVE_PATH}/bertopic_model\")\n",
        "\n",
        "# Save UMAP model\n",
        "joblib.dump(umap_model, f\"{SAVE_PATH}/umap_model.pkl\")\n",
        "\n",
        "# Save topic assignments\n",
        "df.to_csv(f\"{SAVE_PATH}/comments_with_topics.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load more libraries for visualization"
      ],
      "metadata": {
        "id": "fwpwpfwKm7Dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "aqOcKD9QgpAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPAyG6oMST6f"
      },
      "outputs": [],
      "source": [
        "# Ensure topics are integers\n",
        "df[\"new_topics\"] = df[\"new_topics\"].astype(int)\n",
        "\n",
        "# Load reduced 2D embeddings (fix missing \"x\" and \"y\")\n",
        "df[\"x\"], df[\"y\"] = reduced_embeddings[:, 0], reduced_embeddings[:, 1]\n",
        "\n",
        "\n",
        "##### Exclude non-meaningful topics\n",
        "\n",
        "# Define topics to exclude from visualization\n",
        "excluded_topics = {-1, 0, 1, 3, 7, 8, 13, 17, 18, 19, 22}\n",
        "\n",
        "# Exclude these topics\n",
        "df_filtered = df[~df[\"new_topics\"].isin(excluded_topics)]\n",
        "#####\n",
        "\n",
        "# Apply a slight logarithmic transformation to both x and y\n",
        "alpha = 0.6  # Adjust between 0 (no log effect) and 1 (full log effect)\n",
        "\n",
        "df_filtered[\"x\"] = alpha * (np.sign(df_filtered[\"x\"]) * np.log1p(np.abs(df_filtered[\"x\"]))) + (1 - alpha) * df_filtered[\"x\"]\n",
        "df_filtered[\"y\"] = alpha * (np.sign(df_filtered[\"y\"]) * np.log1p(np.abs(df_filtered[\"y\"]))) + (1 - alpha) * df_filtered[\"y\"]\n",
        "\n",
        "\n",
        "# Add jitter\n",
        "jitter_strength = 0.6  # Adjust this value for more or less jiggle\n",
        "df_filtered[\"x\"] += np.random.normal(0, jitter_strength, len(df_filtered))\n",
        "df_filtered[\"y\"] += np.random.normal(0, jitter_strength, len(df_filtered))\n",
        "\n",
        "### Normalize and Rescale UMAP Embeddings\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))  # Normalize x and y to fit between -1 and 1\n",
        "df_filtered[[\"x\", \"y\"]] = scaler.fit_transform(df_filtered[[\"x\", \"y\"]])\n",
        "\n",
        "scaling_factor = 0.5  # to condense space\n",
        "df_filtered[\"x\"] *= scaling_factor\n",
        "df_filtered[\"y\"] *= scaling_factor\n",
        "\n",
        "### Re-center the image by excluding extreme outliers\n",
        "# Compute the 10th percentile of x-values\n",
        "x_lower = np.percentile(df_filtered[\"x\"], 0.2)  # Find the threshold\n",
        "\n",
        "# Filter out the first 10% of x-values\n",
        "df_filtered = df_filtered[df_filtered[\"x\"] > x_lower]\n",
        "\n",
        "# Get the 25 most prevalent topics (excluding the specified ones)\n",
        "top_25_topics = df_filtered[\"new_topics\"].value_counts().index[:25]\n",
        "df_filtered = df_filtered[df_filtered[\"new_topics\"].isin(top_25_topics)]\n",
        "\n",
        "\n",
        "# Get topic frequencies\n",
        "topic_counts = df_filtered[\"new_topics\"].value_counts().to_dict()\n",
        "\n",
        "# ðŸŽ¨ Use multiple high-contrast color palettes to get 50+ distinct colors\n",
        "palette = (\n",
        "    sns.color_palette(\"tab20\", 20) + # Combine color palettes\n",
        "    sns.color_palette(\"Dark2\", 5)\n",
        "\n",
        ")\n",
        "\n",
        "# Ensure we only use the needed number of colors\n",
        "palette = palette[:len(top_25_topics)]  # Trim or extend to match topic count\n",
        "\n",
        "# Create a more compact figure\n",
        "fig, ax = plt.subplots(figsize=(10, 7), dpi=300)  # Reduced figure size for more compact layout\n",
        "\n",
        "# KDE (density visualization) with more emphasis on frequent topics\n",
        "max_count = max(topic_counts.values())  # Get max topic size\n",
        "\n",
        "for topic in top_25_topics:\n",
        "    topic_data = df_filtered[df_filtered[\"new_topics\"] == topic]\n",
        "    topic_size = topic_counts[topic]  # Get size of topic\n",
        "\n",
        "    # Skip topics with too few unique points\n",
        "    if len(topic_data[\"x\"].unique()) < 5 or len(topic_data[\"y\"].unique()) < 5:\n",
        "        continue\n",
        "\n",
        "    # Dynamically adjust KDE density scaling\n",
        "    bw_factor = 0.7 * (topic_size / max_count) + 0.5  # More smoothing for smaller topics\n",
        "\n",
        "    sns.kdeplot(\n",
        "        x=topic_data[\"x\"], y=topic_data[\"y\"],\n",
        "        ax=ax, fill=True, alpha=0.1 + 0.2 * (topic_size / max_count),  # Larger topics get stronger alpha\n",
        "        cmap=sns.light_palette(palette[top_50_topics.tolist().index(topic)], as_cmap=True),\n",
        "        levels=3,  # More levels for larger topics\n",
        "        bw_adjust=bw_factor,  # Adjust KDE smoothing dynamically\n",
        "        thresh=0.05  # Avoids gaps\n",
        "    )\n",
        "\n",
        "\n",
        "# Scatter plot for topic clusters with dynamic size based on topic frequency\n",
        "df_filtered[\"point_size\"] = df_filtered[\"new_topics\"].map(topic_counts.get)\n",
        "df_filtered[\"point_size\"] = df_filtered[\"point_size\"].fillna(df_filtered[\"point_size\"].mean()).astype(int)\n",
        "\n",
        "# Increased size range: minimum size 2, maximum size 20\n",
        "df_filtered[\"point_size\"] = df_filtered[\"point_size\"].apply(lambda x: 2 + 25 * (x / max_count))\n",
        "\n",
        "scatter = sns.scatterplot(\n",
        "    x=\"x\", y=\"y\", hue=\"new_topics\", palette=palette, data=df_filtered,\n",
        "    alpha=0.3, size=\"point_size\", edgecolor=\"w\", ax=ax,\n",
        "    rasterized=True\n",
        ")\n",
        "\n",
        "# Define new labels for selected topics\n",
        "topic_labels = {\n",
        "    2: \"cuteness\\nexpressions\",\n",
        "    #5: \"cuteness expressions (cuteeee)\",\n",
        "    9: \"outfits\",\n",
        "    10: \"babies and pregnancy\",\n",
        "    24: \"family\",\n",
        "    43: \"videos\",\n",
        "    12: \"beauty\",\n",
        "    6: \"tiktok\",\n",
        "    34: \"pets\",\n",
        "    11: \"music\",\n",
        "    21: \"crying\",\n",
        "\n",
        "}\n",
        "\n",
        "fixed_fontsize = 9  # Set a constant font size for all topic labels\n",
        "\n",
        "for topic, label in topic_labels.items():  # Only iterate through topics in topic_labels\n",
        "    topic_data = df_filtered[df_filtered[\"new_topics\"] == topic]\n",
        "    if not topic_data.empty:\n",
        "        x_mean, y_mean = topic_data[\"x\"].median(), topic_data[\"y\"].median()\n",
        "\n",
        "        ax.text(\n",
        "            x_mean, y_mean, label,\n",
        "            fontsize=fixed_fontsize,\n",
        "            fontfamily=\"serif\",\n",
        "            weight=\"semibold\",\n",
        "            ha=\"center\", va=\"center\", color=\"black\",\n",
        "            bbox=dict(facecolor=\"white\", edgecolor=\"none\", alpha=0.6, boxstyle=\"round,pad=0.3\")\n",
        "        )\n",
        "\n",
        "\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "ax.grid(False)\n",
        "ax.legend([], [], frameon=False) # no legend\n",
        "\n",
        "# Finer lines and ticks\n",
        "ax.tick_params(axis=\"both\", which=\"major\", labelsize=7, length=3, width=0.6)\n",
        "ax.tick_params(axis=\"both\", which=\"minor\", labelsize=6, length=2, width=0.4)\n",
        "\n",
        "for spine in ax.spines.values():\n",
        "    spine.set_linewidth(0.4)\n",
        "\n",
        "ax.set_xticks([0.1, 0.5])\n",
        "ax.set_yticks([0.1, 0.5])\n",
        "\n",
        "plt.rc(\"axes\", labelsize=8)\n",
        "plt.rc(\"xtick\", labelsize=7)\n",
        "plt.rc(\"ytick\", labelsize=7)\n",
        "\n",
        "\n",
        "# Add padding\n",
        "plt.subplots_adjust(left=0.12, right=0.88, top=0.92, bottom=0.12)  # Increased top padding\n",
        "\n",
        "# Save visualization\n",
        "plt.savefig(f\"{SAVE_PATH}/prominent_topic_visualization_log.pdf\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MGeOZ9C6hO3",
        "outputId": "520cf76b-7a5f-4ca8-e384-443ecfacc6b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”¹ **Topics & Their Representative Words in the Graph:**\n",
            "Topic 0: awwwww, awwww, awwwwww, wow, awww\n",
            "Topic 2: cute, so, byba, massaallah, ooow\n",
            "Topic 4: cute, so, damn, yeah, \n",
            "Topic 5: cutee, cuteeee, cuteeeee, cuteee, cuteeeeee\n",
            "Topic 6: tiktok, tiktoks, tiktoker, tiktokers, tictok\n",
            "Topic 9: dressing, crop, clothes, dress, dressed\n",
            "Topic 10: cute, usual, stuff, different, as\n",
            "Topic 11: song, music, tune, songs, lyrics\n",
            "Topic 12: pretties, hoe, coffeeformycream, neff, zoooo\n",
            "Topic 15: food, eat, touched, hungry, eating\n",
            "Topic 16: wow, wowl, ln, uhm, there\n",
            "Topic 20: sis, sister, sisters, sissy, big\n",
            "Topic 21: cry, cries, crys, crying, cried\n",
            "Topic 23: gymnastics, gymnast, gymnastic, coach, gymnasts\n",
            "Topic 24: babys, hanging, baby, dear, boinggg\n",
            "Topic 27: trigg, triggs, triggie, nos, skincare\n",
            "Topic 29: father, dad, great, respect, fatherhood\n",
            "Topic 31: nice, niceeeee, nicedo, eeh, waoooooo\n",
            "Topic 30: awww, bur, standards, emmy, raise\n",
            "Topic 32: momma, mama, mamas, mommas, doing\n",
            "Topic 34: pet, ducklings, update, duckys, paddling\n",
            "Topic 35: early, asf, ish, aha, ahaha\n",
            "Topic 37: first, hesitation, yayyy, place, legit\n",
            "Topic 39: watched, times, watch, watching, vpr\n",
            "Topic 42: kids, many, having, kidsi, children\n",
            "Topic 43: videos, feedback, whach, watching, cheers\n",
            "Topic 44: birthday, happy, nele, babygirlll, pootyy\n",
            "Topic 45: growing, fast, grown, grew, grows\n",
            "Topic 47: said, deny, bluff, stupid, pussy\n",
            "Topic 49: bobs, cute, she, soso, shines\n"
          ]
        }
      ],
      "source": [
        "# Get topic words for the topics displayed in the graph\n",
        "topic_words_dict = {}\n",
        "\n",
        "for topic in top_25_topics:\n",
        "    if topic in topic_model.get_topics():\n",
        "        words = topic_model.get_topic(topic)\n",
        "        word_list = [w[0] for w in words]\n",
        "        topic_words_dict[topic] = word_list[:5]  # Get top 5 words for each topic\n",
        "\n",
        "# Print topic labels and corresponding words\n",
        "print(\"Topics & Their Representative Words in the Graph:\")\n",
        "for topic, words in topic_words_dict.items():\n",
        "    print(f\"Topic {topic}: {', '.join(words)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr38KG9Ou28F"
      },
      "outputs": [],
      "source": [
        "# Get topic information for full BERTopic\n",
        "topic_info = topic_model.get_topic_info()\n",
        "\n",
        "# Print first 50 topics\n",
        "print(\"First 50 topics with frequency and keywords:\")\n",
        "for i in range(68):\n",
        "    topic_id = topic_info.iloc[i][\"Topic\"]\n",
        "    freq = topic_info.iloc[i][\"Count\"]\n",
        "    words = topic_info.iloc[i][\"Representation\"]\n",
        "\n",
        "    print(f\"Topic {topic_id}: ({freq} occurrences)\")\n",
        "    print(f\"   Keywords: {', '.join(words)}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}